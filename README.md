# ğŸš€ SMA-KMS

SMA-KMS ist ein **lokal betriebenes**, KI-gestÃ¼tztes Wissensmanagementsystem (KMS), das wissenschaftliche Dokumente ğŸ“„, persÃ¶nliche Notizen ğŸ“ und externe Webquellen ğŸŒ integriert, um effiziente Recherchen und Analysen zu ermÃ¶glichen. Es nutzt moderne Open-Source-Technologien wie Large Language Models (LLMs) und Vektorendatenbanken.  Der Hauptanwendungsbereich ist das Information Security Management, insbesondere fÃ¼r Compliance, wobei es zur Verwaltung sicherheitsrelevanter Informationen und zur UnterstÃ¼tzung von Sicherheitsanalysen eingesetzt werden kann.

---

## ğŸ›  Installation

### ğŸš€ Schnelle Einrichtung mit Docker

#### âœ… Voraussetzungen

- ğŸ–¥ **Git**
- ğŸ³ **Docker & Docker Compose**
- ğŸ§  **laufende Ollama-Instanz** (Installation [hier](https://ollama.ai) verfÃ¼gbar)

1. **Klone das Repository:**

   ```bash
   git clone https://github.com/leonstee/sma-kms.git
   cd sma-kms
   ```

2. **Kopiere die ********************`.env.example`******************** Datei und passe sie an:**

   ```bash
   cp .env.example .env
   ```

   Die folgenden Variablen **mÃ¼ssen** angepasst werden:

   - `ZOTERO_STORAGE_FOLDER` (Pfad zum Zotero-Speicherordner)
   - `OBSIDIAN_MD_FOLDER` (Pfad zum Obsidian-Markdown-Ordner)
   - `LOCAL_PDF_FOLDER` (Pfad zu allen gewÃ¼nschten lokalen PDFs)

   Die folgenden Variablen **kÃ¶nnen** je nach Bedarf angepasst werden:

   - `OLLAMA_HOST` (Ollama-Adresse, falls nicht der Standardport genutzt wird)
   - `LM_MODEL` (verwendetes Large Language Model)
   - `EMBEDDING_MODEL` (Modell zur Vektorisierung von Texten)
   - `QDRANT_COLLECTION` (Name der Sammlung in der Vektordatenbank)
  
   Die Variable `QDRANT_URL` wird im Docker-Betrieb nicht genutzt.
  
3. **Lade das empfohlene Language Model herunter und starte Ollama:**

   ```bash
   ollama pull qwen2.5:7b-instruct
   ollama pull bge-m3
   ollama serve
   ```

4. **Erstelle und starte die Container:**

   ```bash
   docker-compose up -d --build
   ```

5. **Ã–ffne die BenutzeroberflÃ¤che:**
   Sobald die Container gestartet sind, kann das Webinterface Ã¼ber `http://localhost:7860` ğŸŒ aufgerufen werden.

---

### ğŸ”§ Einrichtung fÃ¼r Entwicklung

#### âœ… Voraussetzungen

- ğŸ–¥ **Git**
- ğŸ **Python 3.8 oder neuer**
- ğŸ“¦ **pip** (fÃ¼r Paketverwaltung)
- ğŸ“‚ **Qdrant-Vektordatenbank** (Installation [hier](https://qdrant.tech/documentation/quickstart/) verfÃ¼gbar)
- ğŸ§  **laufende Ollama-Instanz** (Installation [hier](https://ollama.ai) verfÃ¼gbar)

Falls du SMA-KMS ohne Docker lokal einrichten mÃ¶chtest, folge diesen Schritten:

1. **Klone das Repository:**

   ```bash
   git clone https://github.com/leonstee/sma-kms.git
   cd sma-kms
   ```

2. ğŸ“¦ Installiere die benÃ¶tigten AbhÃ¤ngigkeiten:

   ```bash
   pip install --upgrade -r requirements.txt
   ```

3. ğŸ“‘ Kopiere die `.env.example` Datei und passe sie an:

   ```bash
   cp .env.example .env
   ```


4. âš™ **Anpassung der ********************************************`config.py`******************************************** Datei:**

   - Falls SMA-KMS innerhalb von Docker betrieben wird, sind die vordefinierten Pfade in `config.py` (`/zotero`, `/obsidian`, `/pdfs`) korrekt.
   - FÃ¼r lokale Entwicklung mÃ¼ssen stattdessen die entsprechenden Umgebungsvariablen aus `.env` genutzt werden. Hierbei sollten die auskommentierten Alternativen in `config.py` aktiviert werden.

5. ğŸ“‚ **Qdrant-Datenbank starten:**
   SMA-KMS benÃ¶tigt eine **Qdrant-Datenbank** fÃ¼r die Vektorensuche. Falls Qdrant nicht bereits als Dienst lÃ¤uft, kann es mit folgendem Docker-Befehl gestartet werden:

   ```bash
   docker run -d --name qdrant -p 6333:6333 qdrant/qdrant
   ```


6. ğŸ“¥ Lade das empfohlene Language Model herunter und starte Ollama:

   ```bash
   ollama pull qwen2.5:7b-instruct
   ollama pull bge-m3
   ollama serve
   ```

7. â–¶ **Starte die Anwendung:**

   ```bash
   python gradio_ui.py
   ```

8. â–¶ **Starte den Datei-Ãœberwachungsdienst:**
   Damit Ã„nderungen an den gespeicherten Dateien automatisch erkannt und verarbeitet werden, muss folgendes Skript ausgefÃ¼hrt werden:

   ```bash
   python watching.py
   ```

9. **Ã–ffne die BenutzeroberflÃ¤che:**
   Sobald die Container gestartet sind, kann das Webinterface Ã¼ber `http://localhost:7860` ğŸŒ aufgerufen werden.

---

## ğŸ“‚ Datenvorbereitung

Nach der Einrichtung mÃ¼ssen bestehende Dateien manuell eingelesen werden. Dazu wird im **Gradio Interface** der Button **"Vorhandene Dateien einlesen"** gedrÃ¼ckt. Danach erkennt das System **neue, geÃ¤nderte oder gelÃ¶schte Dateien** automatisch und verarbeitet diese entsprechend. âœ…

---

## ğŸŒŸ Vorteile von lokalem Betrieb & Open Source Technologien

### ğŸ” **Datenschutz & Sicherheit**

Da das System vollstÃ¤ndig lokal betrieben wird, bleiben alle Daten auf dem eigenen Rechner und werden nicht in die Cloud hochgeladen. Dies stellt sicher, dass sensible Informationen geschÃ¼tzt bleiben und keine AbhÃ¤ngigkeit von externen Diensten besteht.

### ğŸ›  **Anpassbarkeit & Kontrolle**

Durch den Einsatz von Open-Source-Technologien kann das System individuell angepasst werden. Benutzer\:innen haben vollstÃ¤ndige Kontrolle Ã¼ber die genutzten Modelle, Datenquellen und Integrationen.

### ğŸš€ **Keine AbhÃ¤ngigkeit von Drittanbietern**

Da keine proprietÃ¤ren Cloud-Dienste benÃ¶tigt werden, kann das System unabhÃ¤ngig von kommerziellen Anbietern betrieben werden. Dies spart Kosten und ermÃ¶glicht langfristige Nachhaltigkeit.

### âš¡ **Performance & Offline-VerfÃ¼gbarkeit**

Lokaler Betrieb ermÃ¶glicht schnelle Antwortzeiten und die MÃ¶glichkeit, das System auch ohne Internetverbindung zu nutzen.

---

## ğŸ”§ MÃ¶gliche Erweiterungen & Verbesserungen

- ğŸ›  **Docker-Compose-Profil fÃ¼r Ollama:** Ein zusÃ¤tzlicher Profilmodus in der `docker-compose.yml`, der Ollama direkt in Docker installiert und automatisch die benÃ¶tigten Images herunterlÃ¤dt.
- ğŸ—‚ **Chat-Kontext-History speichern:** FÃ¼r eine kohÃ¤rente und kontextbezogene Interaktion mit dem System.
- ğŸ› **Web-Interface fÃ¼r Einstellungen:** Anpassungen wie Priorisierung von Quellen oder Dateipfade Ã¼ber eine intuitive BenutzeroberflÃ¤che ermÃ¶glichen.
- ğŸ“¤ **In-Chat Datei-Upload:** Direkte Verarbeitung hochgeladener Dokumente mithilfe einer In-Memory-Datenbank.
- ğŸ **Optimierung des Embedding-Modells:** Verbesserung der Suchgenauigkeit und Effizienz durch feinere Abstimmung der Embeddings.
- ğŸ–¼ **OCR- und Bildanalyse:** Integration zur Verarbeitung und Durchsuchbarkeit von gescannten Dokumenten und Bildern.
- ğŸ”„ **Effiziente Vektoren-Aktualisierung:** Ã„nderungen an Dateien automatisch erkennen und aktualisieren, anstatt komplette Neuberechnungen durchzufÃ¼hren.
- ğŸ¨ **Verbesserte UI/UX:** Eine moderne und intuitive BenutzeroberflÃ¤che fÃ¼r eine angenehmere Nutzung.

## ğŸ›  Typische Fehler & deren Behebung

### âŒ Fehler: Ollama lÃ¤uft nicht

**LÃ¶sung:**

- PrÃ¼fe, ob der Ollama-Dienst aktiv ist:
  ```bash
  ps aux | grep ollama
  ```
- Falls Ollama nicht lÃ¤uft, starte den Dienst mit:
  ```bash
  ollama serve
  ```
- Falls Ollama nicht lÃ¤uft, aber nicht erreichbar ist:
     - ÃœberprÃ¼fe mit `ollama serve`, ob Ollama den Standardport 11434 nutzt.
     - Passe gegebenenfalls `OLLAMA_HOST` in der `.env`-Datei an
- Falls der Fehler weiterhin besteht, installiere Ollama neu ([Installationsseite](https://ollama.ai)).

### âŒ Fehler: Docker-Netzwerk "kms" wurde nicht automatisch erstellt

**LÃ¶sung:**

- PrÃ¼fe, ob das Netzwerk existiert:
  ```bash
  docker network ls | grep kms
  ```
- Falls es fehlt, erstelle das Netzwerk manuell:
  ```bash
  docker network create kms
  ```
- Starte anschlieÃŸend die Container neu:
  ```bash
  docker-compose down && docker-compose up -d --build
  ```

### âŒ Fehler: Container starten nicht oder stÃ¼rzen ab

**LÃ¶sung:**

- PrÃ¼fe die Logs mit:
  ```bash
  docker logs container_name
  ```
- Stelle sicher, dass keine Ports blockiert sind (`docker ps` zeigt laufende Container an).
- Versuche, alle Container neu zu starten:
  ```bash
  docker-compose down && docker-compose up -d --build
  ```

### âŒ Fehler: `ollama`-Befehl wird nicht erkannt

**LÃ¶sung:**

- Stelle sicher, dass **Ollama** installiert ist ([Installationsseite](https://ollama.ai)).
- FÃ¼hre `ollama serve` aus, um den Dienst zu starten.

### âŒ Fehler: Qdrant-Datenbank nicht erreichbar

**LÃ¶sung:**

- PrÃ¼fe, ob Qdrant lÃ¤uft:
  ```bash
  docker ps | grep qdrant
  ```
- Falls nicht, starte Qdrant manuell:
  ```bash
  docker run -d --name qdrant -p 6333:6333 qdrant/qdrant
  ```

### âŒ Fehler: `python watching.py` erkennt keine Ã„nderungen

**LÃ¶sung:**

- Stelle sicher, dass die `.env`-Variablen korrekt gesetzt sind.
- PrÃ¼fe, ob die zu Ã¼berwachenden Ordner existieren und korrekt angegeben sind (`ZOTERO_STORAGE_FOLDER`, `OBSIDIAN_MD_FOLDER`, `LOCAL_PDF_FOLDER`).

### âŒ Fehler: Fehlende oder falsche Antworten des Chatbots

**LÃ¶sung:**

- Stelle sicher, dass `ollama serve` lÃ¤uft und die richtigen Modelle geladen wurden.
- ÃœberprÃ¼fe, ob Qdrant korrekt eingerichtet wurde und eine Indexierung stattgefunden hat.
- ÃœberprÃ¼fe im [Qdrant-Dashboard](http://localhost:6333/dashboard), ob die Collection angelegt und EintrÃ¤ge angelegt wurden.
- Falls nÃ¶tig, befÃ¼lle die Datenbank erneut mitÂ einem Klick auf den Button "vorhandene Dateien Einlesen" im [Chat-Interface](http://localhost:7860).

## ğŸ¯ Fazit

SMA-KMS bietet eine **leistungsfÃ¤hige, lokal betriebene** LÃ¶sung fÃ¼r KI-gestÃ¼tztes Wissensmanagement mit **hoher Anpassbarkeit** und **Datenschutz**. Durch den Einsatz moderner **Open-Source-Technologien** ist das System **flexibel erweiterbar** und praxisnah nutzbar. ğŸ†



